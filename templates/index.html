<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="{{url_for('static', filename='style3.css')}}">
    <title>AI Assignment Webpage</title>
</head>
<body>
    <!-- Assignment1 -->
    <div class="container">
        <div class="content">
            <!-- Past Content -->
            <p><strong>1. How do cloud platforms enable the storage and processing of large datasets required for real-world AI projects?</strong></p>
            <p>
                Cloud platforms enable the storage and processing of large datasets for AI projects through scalable storage solutions, 
                elastic compute resources, and big data frameworks. They offer managed AI/ML services for model development and deployment, 
                ensuring security, compliance, and cost-efficiency with pay-as-you-go models. Collaboration tools and APIs further streamline 
                access and integration, allowing teams to focus on innovation without managing complex infrastructure.
            </p>

            <p><strong>2. Why are web applications essential for providing user-friendly interfaces to deploy and interact with AI models?</strong></p>
            <p>
                Web applications are essential for providing user-friendly interfaces to deploy and interact with AI models because they make 
                complex AI functionalities accessible to a broad audience without requiring technical expertise. Web apps are platform-independent, 
                accessible via browsers, and scalable, enabling seamless integration of AI workflows across devices. They also support real-time 
                interactions and updates, enhancing usability and engagement while bridging the gap between technical AI systems and end-users.
            </p>

            <p><strong>3. How do cloud services support the scalability of AI projects when handling high volumes of real-time data?</strong></p>
            <p>
                Cloud services support the scalability of AI projects handling high volumes of real-time data through elastic compute resources, 
                distributed storage, and advanced networking. They dynamically scale infrastructure to accommodate fluctuating workloads, ensuring 
                performance during data spikes. Managed services like serverless computing and real-time data pipelines process and analyze streaming 
                data efficiently. Additionally, auto-scaling and load-balancing features optimize resource usage, while global data centers reduce 
                latency, enabling seamless, cost-effective scalability for AI-driven real-time applications.
            </p>

            <p><strong>4. How do web applications facilitate the integration of AI models into end-user environments for real-world applications?</strong></p>
            <p>
                Web applications facilitate the integration of AI models into end-user environments by providing accessible, interactive interfaces 
                that bridge technical AI capabilities with user-friendly experiences. They enable real-time data input, processing, and output visualization, 
                making AI functionality easy to use without specialized knowledge. Web apps support seamless integration with existing systems via APIs 
                and are platform-independent, allowing deployment across devices. They also ensure scalability, security, and updates, making AI models 
                practical for diverse real-world applications such as e-commerce, healthcare, and automation.
            </p>

            <p><strong>5. How do cloud-based AI services simplify model training, deployment, and monitoring in real-world scenarios?</strong></p>
            <p>
                Cloud-based AI services simplify model training, deployment, and monitoring by providing integrated tools and managed infrastructure. 
                For training, they offer scalable compute resources and frameworks for data preprocessing and model optimization. Deployment is 
                streamlined with services like serverless platforms and container orchestration, ensuring models are accessible globally with low latency. 
                Monitoring tools provide real-time insights into model performance, enabling quick troubleshooting and updates. These services reduce 
                complexity, cost, and time, allowing teams to focus on innovation rather than infrastructure management.
            </p>
        </div>
    </div>

    <!-- New Financial Portal Section -->
    <div class="container">
        <h2>Financial Portal: About Text Classification </h2>

        <!-- Buttons for Navigation -->
        <div class="button-container">
            <button onclick="showSection('section1')">Text Classification</button>
            <button onclick="showSection('section2')">Preprocessing Steps</button>
            <button onclick="showSection('section3')">Machine Learning Algorithms</button>
            <button onclick="showSection('section4')">Challange in Classes</button>
            <button onclick="showSection('section5')">Transfer Learning</button>
        </div>

        <!-- Content Sections -->
        <div id="section1" class="section">
            <h3>1. What is text classification, and why is it an essential task in natural language processing (NLP)? Provide examples of real-world applications where text classification is valuable.</h3>
            <p>
                Text classification is the process of assigning predefined categories or labels to text data. It is an essential task in NLP because it enables the automated understanding and organization of textual information, making it possible to process large volumes of unstructured text efficiently.
                Real-world application:
                <ul>
                    <li>Sentiment analysis: Classifying user reviews as positive, negative, or neutral, often used for product or brand feedback.</li>
                    <li>Topic categorization: Assigning topics to news articles or research papers (e.g., sports, politics, health).</li>
                    <li>Customer support automation: Categorizing customer queries for efficient routing to support agents.</li>
                </ul>
            </p>
        </div>

        <div id="section2" class="section" style="display: none;">
            <h3>2. Describe the preprocessing steps involved in preparing text data for classification. Explain techniques for feature extraction, such as TF-IDF, BERT, fastTEXT and word embeddings.</h3>
            <p>
                Steps:
                <ul>
                    <li>Text cleaning: Removing irrelevant elements like punctuation, stopwords, or special characters.</li>
                    <li>Tokenization: Splitting text into smaller units (tokens), such as words or subwords.</li>
                    <li>Normalization: Lowercasing text and handling abbreviations or stemming/lemmatization.</li>
                </ul>
                Techniques for feature extraction:
                <ul>
                    <li>TF-IDF: Converts text into numerical features by calculating the term frequency (TF) multiplied by the inverse document frequency (IDF). It highlights important words in a document relative to a corpus.</li>
                    <li>BERT: A deep contextualized embedding model that considers the meaning of words based on their surrounding context, offering state-of-the-art text representation.</li>
                    <li>fastText: An extension of Word2Vec that includes subword information for better handling of rare and out-of-vocabulary words.</li>
                    <li>Word embeddings: Techniques like Word2Vec and GloVe map words to dense vectors representing their meaning in context.</li>
                </ul>
            </p>
        </div>

        <div id="section3" class="section" style="display: none;">
            <h3>3. Discuss common machine learning algorithms used for text classification, such as regression and decision tree. How are metrics like accuracy, precision, recall, and F1-score used to evaluate model performance?</h3>
            <p>
                Common machine learning algorithms:
                <ul>
                    <li>Logistic Regression: Suitable for binary classification tasks. It predicts probabilities and is easy to implement.</li>
                    <li>Decision Trees: Splits data based on feature conditions but can overfit without proper pruning.</li>
                    <li>Support Vector Machines (SVM): Effective for high-dimensional data, often used for text data with sparse features.</li>
                </ul>
                Evaluation metrics::
                <ul>
                    <li>Accuracy: The percentage of correct predictions.</li>
                    <li>Precision: The proportion of true positives out of all predicted positives.</li>
                    <li>Recall: The proportion of true positives out of all actual positives.</li>
                    <li>F1-score: The harmonic mean of precision and recall, balancing the trade-off between them.</li>
                </ul>
            </p>
        </div>

        <div id="section4" class="section" style="display: none;">
            <h3>4. Explain the challenge of imbalanced classes in text classification. What strategies can be employed to address class imbalance and maintain model effectiveness?</h3>
            <p>
                Challenge: In imbalanced datasets, one class dominates the others, which can lead to biased models that predict the majority class more often.
                Strategies:
                <ul>
                    <li>Synthetic data generation: Using methods like SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic minority class examples.</li>
                    <li>Class weighting: Assigning higher weights to the minority class during training.</li>
                    <li>Data augmentation: Expanding the dataset by paraphrasing or generating synthetic text.</li>
                    <li>Advanced algorithms: Models like BERT can handle class imbalance better due to their pretraining on diverse data.</li>
                </ul>
            </p>
        </div>

        <div id="section5" class="section" style="display: none;">
            <h3>5. Transfer Learning and Pretrained Models: What is transfer learning in the context of text classification? How can pre-trained language models like BERT and GPT enhance classification tasks?</h3>
            <p>
                Transfer learning involves leveraging knowledge from a pre-trained model to solve a related but different task. It reduces the need for large labeled datasets and accelerates training.
                The advantages of pre-trained models:
                <ul>
                    <li>Contextual embeddings: Capture nuanced meanings of words based on their usage.</li>
                    <li>Efficiency: Pre-trained models reduce the computational cost and time for training.</li>
                    <li>Generalization: They generalize well to various NLP tasks due to their extensive pretraining on massive datasets.</li>
                </ul>
            </p>
        </div>
    </div>

    <script>
        function showSection(sectionId) {
            // Hide all sections
            const sections = document.querySelectorAll('.section');
            sections.forEach(section => section.style.display = 'none');

            // Show selected section
            document.getElementById(sectionId).style.display = 'block';
        }
    </script>
</body>
</html>
